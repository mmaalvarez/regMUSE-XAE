{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d0e63af",
   "metadata": {},
   "source": [
    "Modified from\n",
    "--\n",
    "https://github.com/compbiomed-unito/MUSE-XAE\n",
    "\n",
    "Main changes\n",
    "--\n",
    " - it accepts any number of variables (i.e. neurons in the input layer) rather that just the 96 trinucleotide SBS\n",
    " - it accepts non-count variables, so loss function is not necessarily Poisson, and normalization is done differently (across-samples standardization) to account for negative values, or not done at all\n",
    " - data augmentation is done a priori by resampling coefficients from their CI80%\n",
    " - a single and unique resampling is used at each epoch\n",
    " - no EarlyStopping (instead I recover the epoch with lowest validation loss)\n",
    " - no mapping of signatures to COSMIC\n",
    " - no non-negative weights constraint in the decoder (kernel_constraint=NonNeg()), since regional signature weights can be negative\n",
    " - each sample's exposure to each signature is obtained\n",
    " - it's possible to change the batch size, the % of samples used for validation, and the dimensions (neurons) of the 1st encoder layer (the 2nd and 3rd are fractions thereof)\n",
    " - parallelization of number of signatures, and all of the previous point, is done externally with Nextflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6118d343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, argparse\n",
    "\n",
    "## load functions from utils.py\n",
    "from utils import load_dataset, train_model, encoder_prediction\n",
    "\n",
    "\n",
    "## pass arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-r', '--filename_real_data', type=str, required=True, help='Real dataset (i.e. before resamplings), all samples')\n",
    "parser.add_argument('-t', '--filename_permuted_data_training', type=str, required=True, help='PERMUTED TRAINING samples multiple tables (output from 1_parse_input.R; .tsv)')\n",
    "parser.add_argument('-v', '--filename_permuted_data_validation', type=str, required=True, help='PERMUTED VALIDATION samples single table (output from 1_parse_input.R; .tsv)')\n",
    "parser.add_argument('--n_signatures', type=int, help='Number of signatures (neurons in the latent space) to explore', default = 2, required=False)\n",
    "parser.add_argument('--epochs', type=int, default=1000, help='Number of epochs; WARNING: there needs to exist in ./documents/ 1 different resampled training table per epoch, with the training filenames reflecting this, i.e. ./documents/*iter[1:epochs]*')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='Batch size')\n",
    "parser.add_argument('--l1_size', type=int, default=128, help='N neurons of the first encoder layer; the 2nd and 3rd are fractions thereof')\n",
    "parser.add_argument('--validation_perc', type=int, default=20, help='% of the total samples that is reserved for validation; this is done a priori with 1_parse_input.R')\n",
    "parser.add_argument('--loss', type=str, default='mean_squared_error', help='Loss function to use in the autoencoder')\n",
    "parser.add_argument('--activation', type=str, default='softplus', help='Activation function')\n",
    "parser.add_argument('--normalization', type=bool, default=True, help='Whether or not to perform standardization of the input layer (coefficients)')\n",
    "parser.add_argument('--inputDir', type=str, default='$PWD/datasets/', help='Directory where input data is stored')\n",
    "parser.add_argument('--outputDir', type=str, default='$PWD/res/', help='Directory to save results')\n",
    "parser.add_argument('--seed', type=int, required=False, help='Specify a seed, otherwise it will be selected based on the current time')\n",
    "\n",
    "if 'ipykernel' in sys.modules: # if interactive, pass values manually\n",
    "    filename_real_data = \"original_coeff.tsv\"\n",
    "    filename_permuted_data_training = \"perm_coeff_iter*_training.tsv\"\n",
    "    filename_permuted_data_validation = \"perm_coeff_validation.tsv\"\n",
    "    n_signatures = 3\n",
    "    epochs = 200\n",
    "    batch_size = 25\n",
    "    l1_size = 200\n",
    "    validation_perc = 30\n",
    "    loss = 'mean_squared_error'\n",
    "    activation = 'softplus'\n",
    "    normalization = True\n",
    "    inputDir = \"$PWD/datasets/\"\n",
    "    outputDir = \"$PWD/res/\"\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "    filename_real_data = args.filename_real_data\n",
    "    filename_permuted_data_training = args.filename_permuted_data_training\n",
    "    filename_permuted_data_validation = args.filename_permuted_data_validation\n",
    "    n_signatures = args.n_signatures\n",
    "    epochs = args.epochs\n",
    "    batch_size = args.batch_size\n",
    "    l1_size = args.l1_size\n",
    "    validation_perc = args.validation_perc\n",
    "    loss = args.loss\n",
    "    activation = args.activation\n",
    "    normalization = args.normalization\n",
    "    inputDir = args.inputDir\n",
    "    outputDir = args.outputDir\n",
    "    \n",
    "\n",
    "## load datasets\n",
    "# they have to be in ./inputDir/validation_perc_{validation_perc}/\n",
    "# n_features indicates the number of input neurons (1 per feature included in regressions)\n",
    "# there must be as many available training files as epochs are specified\n",
    "real_data_df,real_data_sample_names,feature_names,n_features,training_validation_dfs_dict,seed = load_dataset(filename_real_data,\n",
    "                                                                                                              filename_permuted_data_training,\n",
    "                                                                                                              filename_permuted_data_validation,\n",
    "                                                                                                              epochs,\n",
    "                                                                                                              validation_perc,\n",
    "                                                                                                              normalization,\n",
    "                                                                                                              inputDir,\n",
    "                                                                                                              seed=None)\n",
    "## create output folder\n",
    "output_folder_name = f'nFeatures_{str(n_features)}__' \\\n",
    "                     f'nSignatures_{str(n_signatures)}__' \\\n",
    "                     f'nEpochs_{str(epochs)}__' \\\n",
    "                     f'batchSize_{str(batch_size)}__' \\\n",
    "                     f'l1Size_{str(l1_size)}__' \\\n",
    "                     f'validationPerc_{str(validation_perc)}__' \\\n",
    "                     f'normalization_{str(normalization)}__' \\\n",
    "                     f'seed_{str(seed)}/'\n",
    "\n",
    "if 'ipykernel' in sys.modules: # if interactive, create folders\n",
    "    if not os.path.exists(outputDir):\n",
    "        os.mkdir(outputDir)\n",
    "    output_folder_name = outputDir + output_folder_name\n",
    "    if not os.path.exists(output_folder_name):\n",
    "        os.mkdir(output_folder_name)\n",
    "else: ## not interactive (nextflow handles the 'outputDir/' folder creation)\n",
    "    os.mkdir(output_folder_name)\n",
    "\n",
    "    \n",
    "## run training with simultaneous validation\n",
    "trained_autoencoder,trained_encoder,loss_plot,signature_weights = train_model(training_validation_dfs_dict = training_validation_dfs_dict,\n",
    "                                                                              input_dim = n_features,\n",
    "                                                                              feature_names = feature_names,\n",
    "                                                                              n_signatures = n_signatures,\n",
    "                                                                              epochs = epochs,\n",
    "                                                                              batch_size = batch_size,\n",
    "                                                                              l1_size = l1_size,\n",
    "                                                                              loss = loss,\n",
    "                                                                              activation = activation,\n",
    "                                                                              seed = seed,\n",
    "                                                                              output_folder_name = output_folder_name)\n",
    "\n",
    "\n",
    "## now use the trained 'encoder' model (already trained via the autoencoder, see above) to encode the original (i.e. not resampled) coefficients matrix into the latent representation\n",
    "encoded_real_df = encoder_prediction(trained_encoder, \n",
    "                                     real_data_df, \n",
    "                                     real_data_sample_names, \n",
    "                                     n_signatures)\n",
    "\n",
    "\n",
    "## save outputs\n",
    "\n",
    "# training performance plot\n",
    "loss_plot.savefig(output_folder_name + 'loss_plot.jpg', dpi=100)    \n",
    "\n",
    "# best encoder model \n",
    "trained_encoder.save(output_folder_name + 'best_encoder_model.tf')\n",
    "\n",
    "# encoded layer (\"signature exposures\")\n",
    "encoded_real_df.to_csv(output_folder_name + 'signature_exposures.tsv', sep='\\t', index= False)\n",
    "\n",
    "# decoder layer weights (\"signature weights\")\n",
    "signature_weights.to_csv(output_folder_name + 'signature_weights.tsv', sep='\\t', index= False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "musexae",
   "language": "python",
   "name": "musexae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

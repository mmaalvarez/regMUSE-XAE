{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b043eeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, argparse\n",
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "## load functions from utils.py\n",
    "from utils import load_dataset, train_model, get_consensus_signatures, encoder_prediction\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "## pass arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-r', '--filename_real_data', type=str, required=True, help='Real dataset (i.e. before resamplings), all samples')\n",
    "parser.add_argument('-t', '--filename_permuted_data_training', type=str, required=True, help='PERMUTED TRAINING samples multiple tables (output from 1_parse_input.R; .tsv)')\n",
    "parser.add_argument('-v', '--filename_permuted_data_validation', type=str, required=True, help='PERMUTED VALIDATION samples single table (output from 1_parse_input.R; .tsv)')\n",
    "parser.add_argument('-k', '--n_signatures', type=int, help='Number of signatures (neurons in the latent space) to explore', default = 2)\n",
    "parser.add_argument('--iters', type=int, default=100, help='Number of independent trainings; then, all the k×iters signatures generated are clustered via K-means into k clusters, and the consensus k signatures are returned')\n",
    "parser.add_argument('--epochs', type=int, default=1000, help='Number of epochs; WARNING: there needs to exist in ./documents/ 1 different resampled training table per epoch, with the training filenames reflecting this, i.e. ./documents/*iter[1:epochs]*')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='Batch size')\n",
    "parser.add_argument('--l1_size', type=int, default=128, help='N neurons of the first encoder layer; the 2nd and 3rd are fractions thereof')\n",
    "parser.add_argument('--validation_perc', type=int, default=20, help='% of the total samples that is reserved for validation; this is done a priori with 1_parse_input.R')\n",
    "parser.add_argument('--loss', type=str, default='mean_squared_error', help='Loss function to use in the autoencoder')\n",
    "parser.add_argument('--activation', type=str, default='softplus', help='Activation function')\n",
    "parser.add_argument('--normalization', type=str, default='yes', help='Whether or not to perform standardization of the input layer (coefficients)')\n",
    "parser.add_argument('--allow_negative_weights', type=str, default='yes', help='If yes (default), negative weights in the decoder layer are allowed. Otherwise, a non-negative constraint is applied to the decoder layer weights')\n",
    "parser.add_argument('--inputDir', type=str, default='$PWD/datasets/', help='Directory where input data is stored')\n",
    "parser.add_argument('--outputDir', type=str, default='$PWD/res/', help='Directory to save results')\n",
    "parser.add_argument('--seed', type=int, required=False, help='Specify a seed, otherwise it will be selected based on the current time')\n",
    "\n",
    "if 'ipykernel' in sys.modules: # if interactive, pass values manually\n",
    "    filename_real_data = \"original_coeff.tsv\"\n",
    "    filename_permuted_data_training = \"perm_coeff_iter*_training.tsv\"\n",
    "    filename_permuted_data_validation = \"perm_coeff_validation.tsv\"\n",
    "    n_signatures = 2\n",
    "    iters = 2\n",
    "    epochs = 100\n",
    "    batch_size = 64\n",
    "    l1_size = 128\n",
    "    validation_perc = 10\n",
    "    loss = 'mean_squared_error'\n",
    "    activation = 'softplus'\n",
    "    normalization = 'no'\n",
    "    allow_negative_weights = 'yes'\n",
    "    inputDir = \"./datasets/\"\n",
    "    outputDir = \"./res/\"\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "    filename_real_data = args.filename_real_data\n",
    "    filename_permuted_data_training = args.filename_permuted_data_training\n",
    "    filename_permuted_data_validation = args.filename_permuted_data_validation\n",
    "    n_signatures = args.n_signatures\n",
    "    iters = args.iters\n",
    "    epochs = args.epochs\n",
    "    batch_size = args.batch_size\n",
    "    l1_size = args.l1_size\n",
    "    validation_perc = args.validation_perc\n",
    "    loss = args.loss\n",
    "    activation = args.activation\n",
    "    normalization = args.normalization\n",
    "    allow_negative_weights = args.allow_negative_weights\n",
    "    inputDir = args.inputDir\n",
    "    outputDir = args.outputDir\n",
    "\n",
    "    \n",
    "## load datasets\n",
    "# they have to be in ./inputDir/validation_perc_{validation_perc}/\n",
    "# n_features indicates the number of input neurons (1 per feature included in regressions)\n",
    "# there must be as many available training files as epochs are specified\n",
    "real_data_df,real_data_sample_names,feature_names,n_features,training_validation_dfs_dict,seed = load_dataset(filename_real_data,\n",
    "                                                                                                              filename_permuted_data_training,\n",
    "                                                                                                              filename_permuted_data_validation,\n",
    "                                                                                                              epochs,\n",
    "                                                                                                              validation_perc,\n",
    "                                                                                                              normalization,\n",
    "                                                                                                              inputDir,\n",
    "                                                                                                              seed=None)\n",
    "## create output folder\n",
    "output_folder_name = f'nFeatures_{str(n_features)}__' \\\n",
    "                     f'nSignatures_{str(n_signatures)}__' \\\n",
    "                     f'nIters_{str(iters)}__' \\\n",
    "                     f'nEpochs_{str(epochs)}__' \\\n",
    "                     f'batchSize_{str(batch_size)}__' \\\n",
    "                     f'l1Size_{str(l1_size)}__' \\\n",
    "                     f'validationPerc_{str(validation_perc)}__' \\\n",
    "                     f'normalization_{str(normalization)}__' \\\n",
    "                     f'allow_negative_weights_{str(allow_negative_weights)}__' \\\n",
    "                     f'seed_{str(seed)}/'\n",
    "\n",
    "if 'ipykernel' in sys.modules: # if interactive, create folders\n",
    "    if not os.path.exists(outputDir):\n",
    "        os.mkdir(outputDir)\n",
    "    output_folder_name = outputDir + output_folder_name\n",
    "    if not os.path.exists(output_folder_name):\n",
    "        os.mkdir(output_folder_name)\n",
    "else: ## not interactive (nextflow handles the 'outputDir/' folder creation)\n",
    "    os.mkdir(output_folder_name)\n",
    "\n",
    "\n",
    "## run training with simultaneous validation\n",
    "\n",
    "# >=1 iterations in parallel\n",
    "results = {}\n",
    "for i in range(iters):\n",
    "    seed = seed+i\n",
    "    results[i] = train_model(training_validation_dfs_dict,\n",
    "                             n_features,\n",
    "                             feature_names,\n",
    "                             n_signatures,\n",
    "                             epochs,\n",
    "                             batch_size,\n",
    "                             l1_size,\n",
    "                             loss,\n",
    "                             activation,\n",
    "                             allow_negative_weights,\n",
    "                             seed,\n",
    "                             output_folder_name,\n",
    "                             i)\n",
    "    \n",
    "extractions = []\n",
    "best_model_losses_epoch_alliters = pd.DataFrame({'output_folder_name': [],\n",
    "                                                 'iter': [],\n",
    "                                                 'seed': [],\n",
    "                                                 'best_model_training_loss': [],\n",
    "                                                 'best_model_validation_loss': [],\n",
    "                                                 'min_val_loss_epoch': []})\n",
    "for i in range(iters):\n",
    "    # get all K×iters signatures (without feature names column)\n",
    "    extractions.append(results[i][3].drop('Feature', axis=1))\n",
    "    # get also the best_model_losses_epoch_i for each iteration\n",
    "    best_model_losses_epoch_alliters = pd.concat([best_model_losses_epoch_alliters, results[i][2]], axis=0)\n",
    "    \n",
    "# store feature names column\n",
    "feature_names_col = results[0][3].loc[:, 'Feature']\n",
    "\n",
    "# get as an example of a loss plot the one from the first iteration\n",
    "loss_plot_first_iter = results[0][1]\n",
    "\n",
    "\n",
    "## cluster all K×iters signatures into K clusters and get the K consensus signatures\n",
    "min_sil, mean_sil, consensus_sig, means_lst = get_consensus_signatures(n_signatures, extractions, feature_names_col)\n",
    "\n",
    "\n",
    "## Refitting: fix the decoder on the consensus signatures, and then train the encoder on the the original (i.e. not resampled) coefficients matrix, to get the sample exposures (latent representation)\n",
    "trained_encoder, encoded_real_df = encoder_prediction(real_data_df=real_data_df,\n",
    "                                                      real_data_sample_names=real_data_sample_names,\n",
    "                                                      S=consensus_sig,\n",
    "                                                      input_dim=n_features,\n",
    "                                                      l1_size=l1_size,\n",
    "                                                      n_signatures=n_signatures,\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      allow_negative_weights=allow_negative_weights,\n",
    "                                                      seed=seed,\n",
    "                                                      output_folder_name=output_folder_name)\n",
    "\n",
    "## save outputs\n",
    "\n",
    "# training performance plot\n",
    "loss_plot_first_iter.savefig(output_folder_name + 'loss_plot_first_iter.jpg', dpi=100)    \n",
    "\n",
    "# save best_model_losses_epoch summary (for all iterations) outside the output folder, nextflow will concatenate all \n",
    "best_model_losses_epoch_alliters.to_csv('best_model_losses_epoch_alliters.tsv', sep='\\t', index= False)\n",
    "\n",
    "# best encoder model \n",
    "trained_encoder.save(output_folder_name + 'best_encoder_model.tf')\n",
    "\n",
    "# encoded layer (\"signature exposures\")\n",
    "encoded_real_df.to_csv(output_folder_name + 'signature_exposures.tsv', sep='\\t', index= False)\n",
    "\n",
    "# decoder layer weights (consensus \"signature weights\")\n",
    "consensus_sig.to_csv(output_folder_name + 'signature_weights.tsv', sep='\\t', index= False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "musexae",
   "language": "python",
   "name": "musexae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
